{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pandas\n",
    "pip install keras\n",
    "pip install opencv-python\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "                                  Image_Path  Num_Faces  \\\n",
      "0  0--Parade/0_Parade_marchingband_1_849.jpg          1   \n",
      "1        0--Parade/0_Parade_Parade_0_904.jpg          1   \n",
      "2  0--Parade/0_Parade_marchingband_1_799.jpg         21   \n",
      "3  0--Parade/0_Parade_marchingband_1_117.jpg          9   \n",
      "4  0--Parade/0_Parade_marchingband_1_778.jpg         35   \n",
      "\n",
      "                                           Face_Info  \n",
      "0             [449, 330, 122, 149, 0, 0, 0, 0, 0, 0]  \n",
      "1              [361, 98, 263, 339, 0, 0, 0, 0, 0, 0]  \n",
      "2  [78, 221, 7, 8, 2, 0, 0, 0, 0, 0, 78, 238, 14,...  \n",
      "3  [69, 359, 50, 36, 1, 0, 0, 0, 0, 1, 227, 382, ...  \n",
      "4  [27, 226, 33, 36, 1, 0, 0, 0, 2, 0, 63, 95, 16...  \n",
      "\n",
      "Testing Data:\n",
      "                                  Image_Path\n",
      "0  0--Parade/0_Parade_marchingband_1_737.jpg\n",
      "1  0--Parade/0_Parade_marchingband_1_494.jpg\n",
      "2        0--Parade/0_Parade_Parade_0_338.jpg\n",
      "3  0--Parade/0_Parade_marchingband_1_533.jpg\n",
      "4   0--Parade/0_Parade_marchingband_1_62.jpg\n",
      "\n",
      "Validation Data:\n",
      "                                   Image_Path  Num_Faces  \\\n",
      "0   0--Parade/0_Parade_marchingband_1_465.jpg        126   \n",
      "1         0--Parade/0_Parade_Parade_0_628.jpg         29   \n",
      "2   0--Parade/0_Parade_marchingband_1_765.jpg        132   \n",
      "3         0--Parade/0_Parade_Parade_0_194.jpg          5   \n",
      "4   0--Parade/0_Parade_marchingband_1_379.jpg         26   \n",
      "5         0--Parade/0_Parade_Parade_0_814.jpg         36   \n",
      "6         0--Parade/0_Parade_Parade_0_470.jpg         25   \n",
      "7  0--Parade/0_Parade_marchingband_1_1045.jpg         22   \n",
      "8   0--Parade/0_Parade_marchingband_1_556.jpg         13   \n",
      "9         0--Parade/0_Parade_Parade_0_829.jpg          1   \n",
      "\n",
      "                                           Face_Info  \n",
      "0  [345, 211, 4, 4, 2, 0, 0, 0, 2, 0, 331, 126, 3...  \n",
      "1  [26, 299, 10, 16, 2, 0, 0, 0, 2, 0, 25, 329, 7...  \n",
      "2  [311, 131, 8, 9, 1, 0, 0, 0, 0, 0, 299, 143, 1...  \n",
      "3  [111, 425, 122, 127, 0, 1, 0, 0, 0, 1, 209, 34...  \n",
      "4  [281, 303, 20, 36, 2, 0, 0, 0, 2, 0, 260, 324,...  \n",
      "5  [74, 417, 8, 9, 2, 0, 0, 0, 1, 0, 54, 394, 4, ...  \n",
      "6  [3, 152, 34, 51, 1, 0, 0, 0, 1, 0, 36, 166, 85...  \n",
      "7  [1, 314, 12, 24, 2, 0, 0, 0, 1, 0, 90, 273, 33...  \n",
      "8  [82, 262, 23, 28, 2, 0, 0, 0, 1, 0, 147, 287, ...  \n",
      "9             [501, 160, 285, 443, 0, 0, 0, 0, 0, 0]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to data files\n",
    "train_data_path = 'C:\\\\Users\\\\alexw\\\\OneDrive\\\\Documents\\\\03_Education\\\\University\\\\University_Programming\\\\Python\\\\Big_Data\\\\Coursework\\\\Datasets\\\\WIDERFACE\\\\wider_face_split\\\\wider_face_train_bbx_gt.txt'\n",
    "test_data_path = 'C:\\\\Users\\\\alexw\\\\OneDrive\\\\Documents\\\\03_Education\\\\University\\\\University_Programming\\\\Python\\\\Big_Data\\\\Coursework\\\\Datasets\\\\WIDERFACE\\\\wider_face_split\\\\wider_face_test_filelist.txt'\n",
    "val_data_path = 'C:\\\\Users\\\\alexw\\\\OneDrive\\\\Documents\\\\03_Education\\\\University\\\\University_Programming\\\\Python\\\\Big_Data\\\\Coursework\\\\Datasets\\\\WIDERFACE\\\\wider_face_split\\\\wider_face_val_bbx_gt.txt'\n",
    "\n",
    "# Function to process training/validation data\n",
    "def process_train_val_data(data_path):\n",
    "    # Read the lines from the text file\n",
    "    with open(data_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    image_paths = []\n",
    "    num_faces_list = []\n",
    "    face_info_list = []\n",
    "\n",
    "    # Process lines to extract information\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Check if it represents an image path\n",
    "        if line.endswith('.jpg'):\n",
    "            image_paths.append(line)\n",
    "            # Initialize num_faces and face_info\n",
    "            num_faces = None\n",
    "            face_info = []\n",
    "        else:\n",
    "            # If it's not an image path, it's face information\n",
    "            if num_faces is None:\n",
    "                # Extract the number of faces\n",
    "                try:\n",
    "                    num_faces = int(line)\n",
    "                except ValueError:\n",
    "                    print(f\"Error parsing number of faces in line: {line}\")\n",
    "                    continue\n",
    "            else:\n",
    "                # Split face information\n",
    "                face_info.extend([int(x) for x in line.split()])\n",
    "\n",
    "            # Check if we have collected enough face information\n",
    "            if len(face_info) == num_faces * 10:\n",
    "                num_faces_list.append(num_faces)\n",
    "                face_info_list.append(face_info)\n",
    "                if len(image_paths) != len(num_faces_list):\n",
    "                    print(f\"Skipping incomplete data point: {image_paths[-1]}\")\n",
    "                    image_paths.pop()  # Remove the corresponding image path\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Image_Path': image_paths,\n",
    "        'Num_Faces': num_faces_list,\n",
    "        'Face_Info': face_info_list\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to process testing data\n",
    "def process_test_data(data_path):\n",
    "    # Read the lines from the text file\n",
    "    with open(data_path, 'r') as file:\n",
    "        test_lines = file.readlines()\n",
    "\n",
    "    # Create a DataFrame with just the image paths\n",
    "    df = pd.DataFrame({\n",
    "        'Image_Path': [line.strip() for line in test_lines],\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process training data\n",
    "train_df = process_train_val_data(train_data_path)\n",
    "\n",
    "# Process testing data\n",
    "test_df = process_test_data(test_data_path)\n",
    "\n",
    "# Process validation data\n",
    "val_df = process_train_val_data(val_data_path)\n",
    "\n",
    "train_df.to_csv('csv\\\\train_data.csv', index=False)\n",
    "test_df.to_csv('csv\\\\test_data.csv', index=False)\n",
    "val_df.to_csv('csv\\\\val_data.csv', index=False)\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"Training Data:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nTesting Data:\")\n",
    "print(test_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "Image_Path    0\n",
      "Num_Faces     0\n",
      "Face_Info     0\n",
      "dtype: int64\n",
      "Preprocessed Training Data:\n",
      "   Num_Faces_Normalized  Num_Faces_Feature\n",
      "0             -0.251883                  1\n",
      "1             -0.251883                  1\n",
      "2              0.190897                 21\n",
      "3             -0.074771                  9\n",
      "4              0.500844                 35\n",
      "Target Variable:\n",
      "0     1\n",
      "1     1\n",
      "2    21\n",
      "3     9\n",
      "4    35\n",
      "Name: Num_Faces, dtype: int64\n",
      "Preprocessed Testing Data:\n",
      "                                    Image_Path  Num_Faces\n",
      "0  0--Parade/0_Parade_marchingband_1_737.jpg\\n        NaN\n",
      "1  0--Parade/0_Parade_marchingband_1_494.jpg\\n        NaN\n",
      "2        0--Parade/0_Parade_Parade_0_338.jpg\\n        NaN\n",
      "3  0--Parade/0_Parade_marchingband_1_533.jpg\\n        NaN\n",
      "4   0--Parade/0_Parade_marchingband_1_62.jpg\\n        NaN\n",
      "Missing Values:\n",
      "Image_Path    0\n",
      "Num_Faces     0\n",
      "Face_Info     0\n",
      "dtype: int64\n",
      "Preprocessed Validation Data:\n",
      "   Num_Faces_Normalized  Num_Faces_Feature\n",
      "0              2.855410                126\n",
      "1              0.419209                 29\n",
      "2              3.006103                132\n",
      "3             -0.183563                  5\n",
      "4              0.343862                 26\n",
      "Target Variable:\n",
      "0    126\n",
      "1     29\n",
      "2    132\n",
      "3      5\n",
      "4     26\n",
      "Name: Num_Faces, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Handle Missing Values\n",
    "    # Check for missing values in the DataFrame\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing Values:\")\n",
    "    print(missing_values)\n",
    "\n",
    "    # Normalize Data\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Normalize the Num_Faces column\n",
    "    df['Num_Faces_Normalized'] = scaler.fit_transform(df[['Num_Faces']])\n",
    "\n",
    "    # Extract Features (Replace this with your feature extraction method)\n",
    "    # For demonstration purposes, let's assume we're using a simple count of faces as a feature\n",
    "    df['Num_Faces_Feature'] = df['Num_Faces']\n",
    "\n",
    "    # Convert Data into Suitable Format\n",
    "    # Assuming we're only using the normalized Num_Faces and the extracted feature as input\n",
    "    X = df[['Num_Faces_Normalized', 'Num_Faces_Feature']]\n",
    "    y = df['Num_Faces']  # Using 'Num_Faces' as the target variable\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Preprocess Training Data\n",
    "X_train, y_train = preprocess_data(train_df)\n",
    "print(\"Preprocessed Training Data:\")\n",
    "print(X_train.head())\n",
    "print(\"Target Variable:\")\n",
    "print(y_train.head())\n",
    "\n",
    "# Read the lines from the test data file\n",
    "with open(test_data_path, 'r') as file:\n",
    "    test_lines = file.readlines()\n",
    "\n",
    "# Preprocess Testing Data\n",
    "test_df = pd.DataFrame({\n",
    "    'Image_Path': test_lines,\n",
    "    'Num_Faces': np.NaN  # Initialize with NaN values\n",
    "})\n",
    "\n",
    "# X_test, y_test = preprocess_data(test_df)\n",
    "print(\"Preprocessed Testing Data:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Preprocess Validation Data\n",
    "X_val, y_val = preprocess_data(val_df)\n",
    "print(\"Preprocessed Validation Data:\")\n",
    "print(X_val.head())\n",
    "print(\"Target Variable:\")\n",
    "print(y_val.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "Image_Path              0\n",
      "Num_Faces               0\n",
      "Face_Info               0\n",
      "Num_Faces_Normalized    0\n",
      "Num_Faces_Feature       0\n",
      "dtype: int64\n",
      "PCA Transformed Data Shape: (12880, 2)\n",
      "t-SNE Transformed Data Shape: (12880, 2)\n",
      "Preprocessed Training Data:\n",
      "   Num_Faces_Normalized  Num_Faces_Feature\n",
      "0             -0.251883                  1\n",
      "1             -0.251883                  1\n",
      "2              0.190897                 21\n",
      "3             -0.074771                  9\n",
      "4              0.500844                 35\n",
      "Target Variable:\n",
      "0     1\n",
      "1     1\n",
      "2    21\n",
      "3     9\n",
      "4    35\n",
      "Name: Num_Faces, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Stage. Validates that there are no missing values. Then standardises the data by scaling it. Then applies dimensionality reduction using PCA and TSNE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Apply Dimensionality Reduction\n",
    "def apply_dimensionality_reduction(X):\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "    return X_pca, X_tsne\n",
    "\n",
    "# Evaluate Dimensionality Reduction Techniques\n",
    "def evaluate_dimensionality_reduction(X, X_pca, X_tsne):\n",
    "    # You can evaluate the techniques using metrics like explained variance, silhouette score, etc.\n",
    "    # For simplicity, we'll just print out the shape of the transformed data\n",
    "    print(\"PCA Transformed Data Shape:\", X_pca.shape)\n",
    "    print(\"t-SNE Transformed Data Shape:\", X_tsne.shape)\n",
    "\n",
    "# Modify the preprocessing function to incorporate dimensionality reduction\n",
    "def preprocess_data(df):\n",
    "    # Handle Missing Values\n",
    "    # Check for missing values in the DataFrame\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing Values:\")\n",
    "    print(missing_values)\n",
    "\n",
    "    # Normalize Data\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Normalize the Num_Faces column\n",
    "    df['Num_Faces_Normalized'] = scaler.fit_transform(df[['Num_Faces']])\n",
    "\n",
    "    # Extract Features (Replace this with your feature extraction method)\n",
    "    # For demonstration purposes, let's assume we're using a simple count of faces as a feature\n",
    "    df['Num_Faces_Feature'] = df['Num_Faces']\n",
    "\n",
    "    # Convert Data into Suitable Format\n",
    "    # Assuming we're only using the normalized Num_Faces and the extracted feature as input\n",
    "    X = df[['Num_Faces_Normalized', 'Num_Faces_Feature']]\n",
    "    y = df['Num_Faces']  # Using 'Num_Faces' as the target variable\n",
    "\n",
    "    # Apply Dimensionality Reduction\n",
    "    X_pca, X_tsne = apply_dimensionality_reduction(X)\n",
    "\n",
    "    # Evaluate Dimensionality Reduction Techniques\n",
    "    evaluate_dimensionality_reduction(X, X_pca, X_tsne)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Preprocess Training Data\n",
    "X_train, y_train = preprocess_data(train_df)\n",
    "print(\"Preprocessed Training Data:\")\n",
    "print(X_train.head())\n",
    "print(\"Target Variable:\")\n",
    "print(y_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "Image_Path        0\n",
      "Num_Faces     16097\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\alexw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1108: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\alexw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1113: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\alexw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1133: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m logreg_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Test the logistic regression model using the test set\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m X_test_processed, y_test_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m logreg_model\u001b[38;5;241m.\u001b[39mpredict(X_test_processed)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 50\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     47\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum_Faces\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Using 'Num_Faces' as the target variable\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Apply Dimensionality Reduction\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m X_pca, X_tsne \u001b[38;5;241m=\u001b[39m \u001b[43mapply_dimensionality_reduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate Dimensionality Reduction Techniques\u001b[39;00m\n\u001b[0;32m     53\u001b[0m evaluate_dimensionality_reduction(X, X_pca, X_tsne)\n",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m, in \u001b[0;36mapply_dimensionality_reduction\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_dimensionality_reduction\u001b[39m(X):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Apply PCA\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     X_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Apply t-SNE\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    279\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\decomposition\\_pca.py:483\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_array_api_compliant:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n\u001b[1;32m--> 483\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1003\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1000\u001b[0m     )\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1003\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the logistic regression model using the test set\n",
    "X_test_processed, y_test_processed = preprocess_data(test_df)\n",
    "y_pred_test = logreg_model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test_processed, y_pred_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "X_val_processed, y_val_processed = preprocess_data(val_df)\n",
    "y_pred_val = logreg_model.predict(X_val_processed)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val_processed, y_pred_val)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
